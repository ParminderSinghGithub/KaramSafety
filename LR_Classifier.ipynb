{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "91aaa0d9-f993-4e34-abf0-5c180d350a0d",
   "metadata": {},
   "source": [
    "Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9a6bdd42-f2bc-44ac-8d40-4f3564664761",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score, confusion_matrix\n",
    "from sklearn.feature_selection import SelectKBest, f_classif, mutual_info_classif, SelectFromModel\n",
    "import joblib\n",
    "from typing import Tuple\n",
    "import warnings\n",
    "from scipy import stats\n",
    "from scipy.fft import fft\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5b0bb2f-4d76-477b-b905-856f73941df5",
   "metadata": {},
   "source": [
    "Data Loading and Batching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b323c400-6d93-473e-a8d4-683ff48b79f2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# class DataLoader:\n",
    "#     def __init__(self, train_split, random_state, batch_size):\n",
    "#         self.train_split = train_split\n",
    "#         self.random_state = random_state\n",
    "#         self.batch_size = batch_size\n",
    "#         self.all_data = None\n",
    "#         self.all_labels = None\n",
    "#         self.file_metadata = None\n",
    "#         self.train_files = None\n",
    "#         self.test_files = None\n",
    "#         self.train_batches = None\n",
    "#         self.test_batches = None\n",
    "#         self.train_batch_labels = None\n",
    "#         self.test_batch_labels = None\n",
    "\n",
    "#     def load_all_data(self, train_dir: str, test_dir: str) -> None:\n",
    "#         all_file_data = {}\n",
    "        \n",
    "#         for directory, dir_type in [(train_dir, 'original_train'), (test_dir, 'original_test')]:\n",
    "#             if not os.path.exists(directory):\n",
    "#                 continue\n",
    "\n",
    "#             for folder in sorted(os.listdir(directory)):\n",
    "#                 folder_path = os.path.join(directory, folder)\n",
    "#                 if not os.path.isdir(folder_path):\n",
    "#                     continue\n",
    "\n",
    "#                 for file in os.listdir(folder_path):\n",
    "#                     if not file.endswith('.csv'):\n",
    "#                         continue\n",
    "\n",
    "#                     file_path = os.path.join(folder_path, file)\n",
    "#                     df = pd.read_csv(file_path, encoding=\"ISO-8859-1\")\n",
    "#                     file_id = f\"{dir_type}_{folder}_{file.replace('.csv', '')}\"\n",
    "                    \n",
    "#                     all_file_data[file_id] = {\n",
    "#                         'data': df,\n",
    "#                         'label': folder,\n",
    "#                         'file_path': file_path,\n",
    "#                         'original_source': dir_type,\n",
    "#                         'activity': folder\n",
    "#                     }\n",
    "\n",
    "#         self.file_metadata = all_file_data\n",
    "#         print(f\"Total files: {len(self.file_metadata)}\")\n",
    "#         print(f\"Activities: {sorted(set([meta['activity'] for meta in self.file_metadata.values()]))}\")\n",
    "        \n",
    "#         self._analyze_file_distribution()\n",
    "#         self._create_batches()\n",
    "\n",
    "#     def _analyze_file_distribution(self):\n",
    "#         print(\"\\nFile distribution per activity:\")\n",
    "#         activity_stats = {}\n",
    "        \n",
    "#         for file_id, meta in self.file_metadata.items():\n",
    "#             activity = meta['activity']\n",
    "#             if activity not in activity_stats:\n",
    "#                 activity_stats[activity] = {'files': 0, 'total_samples': 0}\n",
    "            \n",
    "#             activity_stats[activity]['files'] += 1\n",
    "#             activity_stats[activity]['total_samples'] += len(meta['data'])\n",
    "        \n",
    "#         for activity in sorted(activity_stats.keys()):\n",
    "#             stats = activity_stats[activity]\n",
    "#             print(f\"{activity}: {stats['files']} files, {stats['total_samples']} samples\")\n",
    "        \n",
    "#         total_files = sum([stats['files'] for stats in activity_stats.values()])\n",
    "#         total_samples = sum([stats['total_samples'] for stats in activity_stats.values()])\n",
    "#         print(f\"Total: {total_files} files, {total_samples} samples\")\n",
    "\n",
    "#     def _create_batches(self):\n",
    "#         print(f\"\\nCreating batches with batch_size={self.batch_size}\")\n",
    "        \n",
    "#         all_batches = []\n",
    "#         all_batch_labels = []\n",
    "#         batch_file_mapping = []\n",
    "        \n",
    "#         for file_id, meta in self.file_metadata.items():\n",
    "#             df = meta['data'].copy()\n",
    "#             label = meta['label']\n",
    "            \n",
    "#             cols_to_exclude = ['Unnamed: 0', 'time', 'target']\n",
    "#             feature_cols = [col for col in df.columns if col not in cols_to_exclude]\n",
    "\n",
    "#             features = df[feature_cols].dropna()\n",
    "            \n",
    "#             if len(features) == 0:\n",
    "#                 continue\n",
    "            \n",
    "#             n_complete_batches = len(features) // self.batch_size\n",
    "            \n",
    "#             if n_complete_batches == 0:\n",
    "#                 print(f\"File {file_id}: {len(features)} samples - No complete batches (skipped)\")\n",
    "#                 continue\n",
    "            \n",
    "#             print(f\"File {file_id}: {len(features)} samples -> {n_complete_batches} batches\")\n",
    "            \n",
    "#             for batch_idx in range(n_complete_batches):\n",
    "#                 start_idx = batch_idx * self.batch_size\n",
    "#                 end_idx = start_idx + self.batch_size\n",
    "                \n",
    "#                 batch_data = features.iloc[start_idx:end_idx].values\n",
    "#                 aggregated_batch = np.concatenate([\n",
    "#                         np.mean(batch_data, axis=0),\n",
    "#                         np.std(batch_data, axis=0),\n",
    "#                         np.min(batch_data, axis=0),\n",
    "#                         np.max(batch_data, axis=0)\n",
    "#                 ])\n",
    "                \n",
    "#                 all_batches.append(aggregated_batch)\n",
    "#                 all_batch_labels.append(label)\n",
    "#                 batch_file_mapping.append(file_id)\n",
    "        \n",
    "#         self.all_batches = np.array(all_batches)\n",
    "#         self.all_batch_labels = np.array(all_batch_labels)\n",
    "#         self.batch_file_mapping = batch_file_mapping\n",
    "        \n",
    "#         print(f\"\\nTotal batches created: {len(self.all_batches)}\")\n",
    "#         print(f\"Batch feature dimension: {self.all_batches.shape[1] if len(self.all_batches) > 0 else 0}\")\n",
    "\n",
    "#     def split_by_files(self) -> Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray]:\n",
    "#         if self.all_batches is None or len(self.all_batches) == 0:\n",
    "#             raise ValueError(\"No batches created. Check if data was loaded properly.\")\n",
    "\n",
    "#         all_files = list(self.file_metadata.keys())\n",
    "#         np.random.seed(self.random_state)\n",
    "#         np.random.shuffle(all_files)\n",
    "\n",
    "#         n_train_files = int(len(all_files) * self.train_split)\n",
    "#         self.train_files = all_files[:n_train_files]\n",
    "#         self.test_files = all_files[n_train_files:]\n",
    "\n",
    "#         print(f\"\\nTrain files: {len(self.train_files)}\")\n",
    "#         print(f\"Test files: {len(self.test_files)}\")\n",
    "\n",
    "#         train_indices = [i for i, file_id in enumerate(self.batch_file_mapping) if file_id in self.train_files]\n",
    "#         test_indices = [i for i, file_id in enumerate(self.batch_file_mapping) if file_id in self.test_files]\n",
    "\n",
    "#         X_train = self.all_batches[train_indices]\n",
    "#         X_test = self.all_batches[test_indices]\n",
    "#         y_train = self.all_batch_labels[train_indices]\n",
    "#         y_test = self.all_batch_labels[test_indices]\n",
    "\n",
    "#         self.train_batches = X_train\n",
    "#         self.test_batches = X_test\n",
    "#         self.train_batch_labels = y_train\n",
    "#         self.test_batch_labels = y_test\n",
    "\n",
    "#         print(f\"Train batches: {len(X_train)}\")\n",
    "#         print(f\"Test batches: {len(X_test)}\")\n",
    "\n",
    "#         self._check_activity_distribution_batches(train_indices, test_indices)\n",
    "\n",
    "#         return X_train, X_test, y_train, y_test\n",
    "\n",
    "#     def _check_activity_distribution_batches(self, train_indices, test_indices):\n",
    "#         train_activities = set(self.all_batch_labels[train_indices])\n",
    "#         test_activities = set(self.all_batch_labels[test_indices])\n",
    "#         all_activities = set(self.all_batch_labels)\n",
    "\n",
    "#         print(f\"Train activities: {sorted(train_activities)}\")\n",
    "#         print(f\"Test activities: {sorted(test_activities)}\")\n",
    "\n",
    "#         if all_activities - train_activities:\n",
    "#             print(f\"Missing in train: {sorted(all_activities - train_activities)}\")\n",
    "#         if all_activities - test_activities:\n",
    "#             print(f\"Missing in test: {sorted(all_activities - test_activities)}\")\n",
    "\n",
    "#         print(\"\\nBatch distribution per activity:\")\n",
    "#         for activity in sorted(all_activities):\n",
    "#             train_count = np.sum(self.all_batch_labels[train_indices] == activity)\n",
    "#             test_count = np.sum(self.all_batch_labels[test_indices] == activity)\n",
    "#             print(f\"{activity}: {train_count} train batches, {test_count} test batches\")\n",
    "\n",
    "#     def load_data(self, root_dir: str) -> Tuple[np.ndarray, np.ndarray]:\n",
    "#         if root_dir.lower() == 'train':\n",
    "#             if self.train_batches is None:\n",
    "#                 raise ValueError(\"Call get_train_test_split first.\")\n",
    "#             return self.train_batches, self.train_batch_labels\n",
    "#         elif root_dir.lower() == 'test':\n",
    "#             if self.test_batches is None:\n",
    "#                 raise ValueError(\"Call get_train_test_split first.\")\n",
    "#             return self.test_batches, self.test_batch_labels\n",
    "#         else:\n",
    "#             raise ValueError(\"Invalid root_dir. Use 'train' or 'test'.\")\n",
    "\n",
    "#     def get_train_test_split(self, train_dir: str = 'data_received_/recorded_data_12344', test_dir: str = 'data_received_/recorded_data_12345') -> Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray]:\n",
    "#         self.load_all_data(train_dir, test_dir)\n",
    "#         return self.split_by_files()\n",
    "\n",
    "# loader = DataLoader(train_split=0.8, random_state=38, batch_size=51)\n",
    "# X_train_batches, X_test_batches, y_train_batches, y_test_batches = loader.get_train_test_split('data_received/recorded_data_12344', 'data_received/recorded_data_12345')\n",
    "\n",
    "# print(f\"\\nFinal Results:\")\n",
    "# print(f\"Train batches shape: {X_train_batches.shape}\")\n",
    "# print(f\"Test batches shape: {X_test_batches.shape}\")\n",
    "# print(f\"Features per batch: {X_train_batches.shape[1] if len(X_train_batches) > 0 else 0}\")\n",
    "# print(f\"Classes: {len(np.unique(y_train_batches)) if len(y_train_batches) > 0 else 0}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "358fb37b-6d8d-4dca-8089-92220360795d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total files: 136\n",
      "Activities: ['idle', 'with_hook_climbing_up', 'with_hook_descending_down', 'without_hook_climbing_up', 'without_hook_descending_down']\n",
      "\n",
      "File distribution per activity:\n",
      "idle: 16 files, 15347 samples\n",
      "with_hook_climbing_up: 24 files, 12223 samples\n",
      "with_hook_descending_down: 25 files, 14334 samples\n",
      "without_hook_climbing_up: 36 files, 14405 samples\n",
      "without_hook_descending_down: 35 files, 13548 samples\n",
      "Total: 136 files, 69857 samples\n",
      "\n",
      "Creating batches with batch_size=51\n",
      "File original_train_idle_18062025_114823_12344_2: 594 samples -> 11 batches\n",
      "File original_train_idle_18062025_114949_12344_3: 835 samples -> 16 batches\n",
      "File original_train_idle_18062025_115127_12344_4: 1290 samples -> 25 batches\n",
      "File original_train_idle_18062025_115503_12344_5: 835 samples -> 16 batches\n",
      "File original_train_idle_18062025_115754_12344_6: 1076 samples -> 21 batches\n",
      "File original_train_idle_18062025_121814_12344_7: 737 samples -> 14 batches\n",
      "File original_train_idle_18062025_141135_12344_7: 710 samples -> 13 batches\n",
      "File original_train_idle_18062025_141314_12344_8: 1597 samples -> 31 batches\n",
      "File original_train_with_hook_climbing_up_18062025_114547_12344_1: 535 samples -> 10 batches\n",
      "File original_train_with_hook_climbing_up_18062025_115048_12344_2: 480 samples -> 9 batches\n",
      "File original_train_with_hook_climbing_up_18062025_115336_12344_3: 433 samples -> 8 batches\n",
      "File original_train_with_hook_climbing_up_18062025_115604_12344_4: 468 samples -> 9 batches\n",
      "File original_train_with_hook_climbing_up_18062025_115725_12344_5: 417 samples -> 8 batches\n",
      "File original_train_with_hook_climbing_up_18062025_120606_12344_6: 946 samples -> 18 batches\n",
      "File original_train_with_hook_climbing_up_18062025_121710_12344_7: 390 samples -> 7 batches\n",
      "File original_train_with_hook_climbing_up_18062025_121941_12344_8: 427 samples -> 8 batches\n",
      "File original_train_with_hook_climbing_up_18062025_122130_12344_9: 460 samples -> 9 batches\n",
      "File original_train_with_hook_climbing_up_18062025_140344_12344_10: 527 samples -> 10 batches\n",
      "File original_train_with_hook_climbing_up_18062025_141450_12344_11: 507 samples -> 9 batches\n",
      "File original_train_with_hook_climbing_up_18062025_141612_12344_12: 520 samples -> 10 batches\n",
      "File original_train_with_hook_descending_down_18062025_114910_12344_1: 544 samples -> 10 batches\n",
      "File original_train_with_hook_descending_down_18062025_115248_12344_2: 600 samples -> 11 batches\n",
      "File original_train_with_hook_descending_down_18062025_115423_12344_3: 590 samples -> 11 batches\n",
      "File original_train_with_hook_descending_down_18062025_115636_12344_4: 569 samples -> 11 batches\n",
      "File original_train_with_hook_descending_down_18062025_115903_12344_5: 530 samples -> 10 batches\n",
      "File original_train_with_hook_descending_down_18062025_120713_12344_6: 860 samples -> 16 batches\n",
      "File original_train_with_hook_descending_down_18062025_121907_12344_7: 481 samples -> 9 batches\n",
      "File original_train_with_hook_descending_down_18062025_122038_12344_8: 498 samples -> 9 batches\n",
      "File original_train_with_hook_descending_down_18062025_122230_12344_9: 475 samples -> 9 batches\n",
      "File original_train_with_hook_descending_down_18062025_141231_12344_10: 596 samples -> 11 batches\n",
      "File original_train_with_hook_descending_down_18062025_141526_12344_11: 572 samples -> 11 batches\n",
      "File original_train_with_hook_descending_down_18062025_141712_12344_12: 583 samples -> 11 batches\n",
      "File original_train_with_hook_descending_down_18062025_141849_12344_13: 540 samples -> 10 batches\n",
      "File original_train_without_hook_climbing_up_18062025_120818_12344_1: 341 samples -> 6 batches\n",
      "File original_train_without_hook_climbing_up_18062025_120945_12344_2: 335 samples -> 6 batches\n",
      "File original_train_without_hook_climbing_up_18062025_121045_12344_3: 359 samples -> 7 batches\n",
      "File original_train_without_hook_climbing_up_18062025_121150_12344_4: 365 samples -> 7 batches\n",
      "File original_train_without_hook_climbing_up_18062025_121251_12344_5: 329 samples -> 6 batches\n",
      "File original_train_without_hook_climbing_up_18062025_121442_12344_6: 341 samples -> 6 batches\n",
      "File original_train_without_hook_climbing_up_18062025_121540_12344_7: 324 samples -> 6 batches\n",
      "File original_train_without_hook_climbing_up_18062025_122309_12344_8: 408 samples -> 8 batches\n",
      "File original_train_without_hook_climbing_up_18062025_122546_12344_9: 430 samples -> 8 batches\n",
      "File original_train_without_hook_climbing_up_18062025_122702_12344_10: 481 samples -> 9 batches\n",
      "File original_train_without_hook_climbing_up_18062025_141937_12344_11: 553 samples -> 10 batches\n",
      "File original_train_without_hook_climbing_up_18062025_142426_12344_12: 467 samples -> 9 batches\n",
      "File original_train_without_hook_climbing_up_18062025_142541_12344_13: 512 samples -> 10 batches\n",
      "File original_train_without_hook_climbing_up_18062025_142706_12344_14: 672 samples -> 13 batches\n",
      "File original_train_without_hook_climbing_up_18062025_142832_12344_15: 281 samples -> 5 batches\n",
      "File original_train_without_hook_climbing_up_18062025_143121_12344_16: 330 samples -> 6 batches\n",
      "File original_train_without_hook_climbing_up_18062025_143217_12344_17: 308 samples -> 6 batches\n",
      "File original_train_without_hook_climbing_up_18062025_143312_12344_18: 366 samples -> 7 batches\n",
      "File original_train_without_hook_descending_down_18062025_120904_12344_1: 353 samples -> 6 batches\n",
      "File original_train_without_hook_descending_down_18062025_121012_12344_2: 342 samples -> 6 batches\n",
      "File original_train_without_hook_descending_down_18062025_121113_12344_3: 350 samples -> 6 batches\n",
      "File original_train_without_hook_descending_down_18062025_121215_12344_4: 370 samples -> 7 batches\n",
      "File original_train_without_hook_descending_down_18062025_121316_12344_5: 326 samples -> 6 batches\n",
      "File original_train_without_hook_descending_down_18062025_121506_12344_6: 332 samples -> 6 batches\n",
      "File original_train_without_hook_descending_down_18062025_121607_12344_7: 317 samples -> 6 batches\n",
      "File original_train_without_hook_descending_down_18062025_122515_12344_8: 293 samples -> 5 batches\n",
      "File original_train_without_hook_descending_down_18062025_122633_12344_9: 406 samples -> 7 batches\n",
      "File original_train_without_hook_descending_down_18062025_122750_12344_10: 399 samples -> 7 batches\n",
      "File original_train_without_hook_descending_down_18062025_142350_12344_11: 536 samples -> 10 batches\n",
      "File original_train_without_hook_descending_down_18062025_142507_12344_12: 503 samples -> 9 batches\n",
      "File original_train_without_hook_descending_down_18062025_142629_12344_13: 551 samples -> 10 batches\n",
      "File original_train_without_hook_descending_down_18062025_142804_12344_14: 434 samples -> 8 batches\n",
      "File original_train_without_hook_descending_down_18062025_143058_12344_15: 342 samples -> 6 batches\n",
      "File original_train_without_hook_descending_down_18062025_143152_12344_16: 369 samples -> 7 batches\n",
      "File original_train_without_hook_descending_down_18062025_143345_12344_17: 383 samples -> 7 batches\n",
      "File original_test_idle_18062025_114826_12345_1: 595 samples -> 11 batches\n",
      "File original_test_idle_18062025_114956_12345_2: 835 samples -> 16 batches\n",
      "File original_test_idle_18062025_115132_12345_3: 1289 samples -> 25 batches\n",
      "File original_test_idle_18062025_115510_12345_4: 834 samples -> 16 batches\n",
      "File original_test_idle_18062025_115759_12345_5: 1077 samples -> 21 batches\n",
      "File original_test_idle_18062025_121818_12345_6: 736 samples -> 14 batches\n",
      "File original_test_idle_18062025_141142_12345_7: 708 samples -> 13 batches\n",
      "File original_test_idle_18062025_141319_12345_8: 1599 samples -> 31 batches\n",
      "File original_test_with_hook_climbing_up_18062025_114558_12345_1: 537 samples -> 10 batches\n",
      "File original_test_with_hook_climbing_up_18062025_115044_12345_2: 481 samples -> 9 batches\n",
      "File original_test_with_hook_climbing_up_18062025_115342_12345_3: 434 samples -> 8 batches\n",
      "File original_test_with_hook_climbing_up_18062025_115600_12345_4: 469 samples -> 9 batches\n",
      "File original_test_with_hook_climbing_up_18062025_115720_12345_5: 417 samples -> 8 batches\n",
      "File original_test_with_hook_climbing_up_18062025_120609_12345_6: 946 samples -> 18 batches\n",
      "File original_test_with_hook_climbing_up_18062025_121702_12345_7: 389 samples -> 7 batches\n",
      "File original_test_with_hook_climbing_up_18062025_121946_12345_8: 427 samples -> 8 batches\n",
      "File original_test_with_hook_climbing_up_18062025_122139_12345_9: 459 samples -> 9 batches\n",
      "File original_test_with_hook_climbing_up_18062025_140334_12345_10: 527 samples -> 10 batches\n",
      "File original_test_with_hook_climbing_up_18062025_141446_12345_11: 508 samples -> 9 batches\n",
      "File original_test_with_hook_climbing_up_18062025_141608_12345_12: 519 samples -> 10 batches\n",
      "File original_test_with_hook_descending_down_18062025_114904_12345_1: 544 samples -> 10 batches\n",
      "File original_test_with_hook_descending_down_18062025_115244_12345_2: 599 samples -> 11 batches\n",
      "File original_test_with_hook_descending_down_18062025_115418_12345_3: 590 samples -> 11 batches\n",
      "File original_test_with_hook_descending_down_18062025_115642_12345_4: 569 samples -> 11 batches\n",
      "File original_test_with_hook_descending_down_18062025_115859_12345_5: 531 samples -> 10 batches\n",
      "File original_test_with_hook_descending_down_18062025_120716_12345_6: 859 samples -> 16 batches\n",
      "File original_test_with_hook_descending_down_18062025_121902_12345_7: 480 samples -> 9 batches\n",
      "File original_test_with_hook_descending_down_18062025_122025_12345_8: 497 samples -> 9 batches\n",
      "File original_test_with_hook_descending_down_18062025_122225_12345_9: 474 samples -> 9 batches\n",
      "File original_test_with_hook_descending_down_18062025_141225_12345_10: 597 samples -> 11 batches\n",
      "File original_test_with_hook_descending_down_18062025_141531_12345_11: 573 samples -> 11 batches\n",
      "File original_test_with_hook_descending_down_18062025_141717_12345_12: 583 samples -> 11 batches\n",
      "File original_test_without_hook_climbing_up_18062025_120821_12345_1: 342 samples -> 6 batches\n",
      "File original_test_without_hook_climbing_up_18062025_120936_12345_2: 334 samples -> 6 batches\n",
      "File original_test_without_hook_climbing_up_18062025_121039_12345_3: 360 samples -> 7 batches\n",
      "File original_test_without_hook_climbing_up_18062025_121146_12345_4: 365 samples -> 7 batches\n",
      "File original_test_without_hook_climbing_up_18062025_121247_12345_5: 329 samples -> 6 batches\n",
      "File original_test_without_hook_climbing_up_18062025_121438_12345_6: 342 samples -> 6 batches\n",
      "File original_test_without_hook_climbing_up_18062025_121535_12345_7: 324 samples -> 6 batches\n",
      "File original_test_without_hook_climbing_up_18062025_122339_12345_8: 408 samples -> 8 batches\n",
      "File original_test_without_hook_climbing_up_18062025_122553_12345_9: 431 samples -> 8 batches\n",
      "File original_test_without_hook_climbing_up_18062025_122707_12345_10: 480 samples -> 9 batches\n",
      "File original_test_without_hook_climbing_up_18062025_141932_12345_11: 553 samples -> 10 batches\n",
      "File original_test_without_hook_climbing_up_18062025_142430_12345_12: 468 samples -> 9 batches\n",
      "File original_test_without_hook_climbing_up_18062025_142545_12345_13: 511 samples -> 10 batches\n",
      "File original_test_without_hook_climbing_up_18062025_142711_12345_14: 672 samples -> 13 batches\n",
      "File original_test_without_hook_climbing_up_18062025_142849_12345_15: 281 samples -> 5 batches\n",
      "File original_test_without_hook_climbing_up_18062025_143126_12345_16: 329 samples -> 6 batches\n",
      "File original_test_without_hook_climbing_up_18062025_143220_12345_17: 308 samples -> 6 batches\n",
      "File original_test_without_hook_climbing_up_18062025_143317_12345_18: 366 samples -> 7 batches\n",
      "File original_test_without_hook_descending_down_18062025_120908_12345_1: 354 samples -> 6 batches\n",
      "File original_test_without_hook_descending_down_18062025_121016_12345_2: 342 samples -> 6 batches\n",
      "File original_test_without_hook_descending_down_18062025_121120_12345_3: 350 samples -> 6 batches\n",
      "File original_test_without_hook_descending_down_18062025_121219_12345_4: 371 samples -> 7 batches\n",
      "File original_test_without_hook_descending_down_18062025_121321_12345_5: 326 samples -> 6 batches\n",
      "File original_test_without_hook_descending_down_18062025_121510_12345_6: 333 samples -> 6 batches\n",
      "File original_test_without_hook_descending_down_18062025_121612_12345_7: 317 samples -> 6 batches\n",
      "File original_test_without_hook_descending_down_18062025_122509_12345_8: 292 samples -> 5 batches\n",
      "File original_test_without_hook_descending_down_18062025_122627_12345_9: 406 samples -> 7 batches\n",
      "File original_test_without_hook_descending_down_18062025_122740_12345_10: 399 samples -> 7 batches\n",
      "File original_test_without_hook_descending_down_18062025_142343_12345_11: 536 samples -> 10 batches\n",
      "File original_test_without_hook_descending_down_18062025_142501_12345_12: 504 samples -> 9 batches\n",
      "File original_test_without_hook_descending_down_18062025_142625_12345_13: 550 samples -> 10 batches\n",
      "File original_test_without_hook_descending_down_18062025_142801_12345_14: 434 samples -> 8 batches\n",
      "File original_test_without_hook_descending_down_18062025_143054_12345_15: 342 samples -> 6 batches\n",
      "File original_test_without_hook_descending_down_18062025_143148_12345_16: 370 samples -> 7 batches\n",
      "File original_test_without_hook_descending_down_18062025_143244_12345_17: 333 samples -> 6 batches\n",
      "File original_test_without_hook_descending_down_18062025_143341_12345_18: 383 samples -> 7 batches\n",
      "\n",
      "Total batches created: 1306\n",
      "Batch feature dimension: 40\n",
      "\n",
      "Train files: 108\n",
      "Test files: 28\n",
      "Train batches: 1007\n",
      "Test batches: 299\n",
      "Train activities: [np.str_('idle'), np.str_('with_hook_climbing_up'), np.str_('with_hook_descending_down'), np.str_('without_hook_climbing_up'), np.str_('without_hook_descending_down')]\n",
      "Test activities: [np.str_('idle'), np.str_('with_hook_climbing_up'), np.str_('with_hook_descending_down'), np.str_('without_hook_climbing_up'), np.str_('without_hook_descending_down')]\n",
      "\n",
      "Batch distribution per activity:\n",
      "idle: 193 train batches, 101 test batches\n",
      "with_hook_climbing_up: 194 train batches, 36 test batches\n",
      "with_hook_descending_down: 185 train batches, 83 test batches\n",
      "without_hook_climbing_up: 222 train batches, 48 test batches\n",
      "without_hook_descending_down: 213 train batches, 31 test batches\n",
      "\n",
      "Final Results:\n",
      "Train batches shape: (1007, 40)\n",
      "Test batches shape: (299, 40)\n",
      "Features per batch: 40\n",
      "Classes: 5\n"
     ]
    }
   ],
   "source": [
    "class DataLoader:\n",
    "    def __init__(self, train_split, random_state, batch_size):\n",
    "        self.train_split = train_split\n",
    "        self.random_state = random_state\n",
    "        self.batch_size = batch_size\n",
    "        self.all_data = None\n",
    "        self.all_labels = None\n",
    "        self.file_metadata = None\n",
    "        self.train_files = None\n",
    "        self.test_files = None\n",
    "        self.train_batches = None\n",
    "        self.test_batches = None\n",
    "        self.train_batch_labels = None\n",
    "        self.test_batch_labels = None\n",
    "\n",
    "    def load_all_data(self, train_dir: str, test_dir: str) -> None:\n",
    "        all_file_data = {}\n",
    "        \n",
    "        for directory, dir_type in [(train_dir, 'original_train'), (test_dir, 'original_test')]:\n",
    "            if not os.path.exists(directory):\n",
    "                continue\n",
    "\n",
    "            for folder in sorted(os.listdir(directory)):\n",
    "                folder_path = os.path.join(directory, folder)\n",
    "                if not os.path.isdir(folder_path):\n",
    "                    continue\n",
    "\n",
    "                for file in os.listdir(folder_path):\n",
    "                    if not file.endswith('.csv'):\n",
    "                        continue\n",
    "\n",
    "                    file_path = os.path.join(folder_path, file)\n",
    "                    df = pd.read_csv(file_path, encoding=\"ISO-8859-1\")\n",
    "                    file_id = f\"{dir_type}_{folder}_{file.replace('.csv', '')}\"\n",
    "                    \n",
    "                    all_file_data[file_id] = {\n",
    "                        'data': df,\n",
    "                        'label': folder,\n",
    "                        'file_path': file_path,\n",
    "                        'original_source': dir_type,\n",
    "                        'activity': folder\n",
    "                    }\n",
    "\n",
    "        self.file_metadata = all_file_data\n",
    "        print(f\"Total files: {len(self.file_metadata)}\")\n",
    "        print(f\"Activities: {sorted(set([meta['activity'] for meta in self.file_metadata.values()]))}\")\n",
    "        \n",
    "        self._analyze_file_distribution()\n",
    "        self._create_batches()\n",
    "\n",
    "    def _analyze_file_distribution(self):\n",
    "        print(\"\\nFile distribution per activity:\")\n",
    "        activity_stats = {}\n",
    "        \n",
    "        for file_id, meta in self.file_metadata.items():\n",
    "            activity = meta['activity']\n",
    "            if activity not in activity_stats:\n",
    "                activity_stats[activity] = {'files': 0, 'total_samples': 0}\n",
    "            \n",
    "            activity_stats[activity]['files'] += 1\n",
    "            activity_stats[activity]['total_samples'] += len(meta['data'])\n",
    "        \n",
    "        for activity in sorted(activity_stats.keys()):\n",
    "            stats = activity_stats[activity]\n",
    "            print(f\"{activity}: {stats['files']} files, {stats['total_samples']} samples\")\n",
    "        \n",
    "        total_files = sum([stats['files'] for stats in activity_stats.values()])\n",
    "        total_samples = sum([stats['total_samples'] for stats in activity_stats.values()])\n",
    "        print(f\"Total: {total_files} files, {total_samples} samples\")\n",
    "\n",
    "    def _create_batches(self):\n",
    "        print(f\"\\nCreating batches with batch_size={self.batch_size}\")\n",
    "        \n",
    "        all_batches = []\n",
    "        all_batch_labels = []\n",
    "        batch_file_mapping = []\n",
    "        \n",
    "        for file_id, meta in self.file_metadata.items():\n",
    "            df = meta['data'].copy()\n",
    "            label = meta['label']\n",
    "            \n",
    "            cols_to_exclude = ['Unnamed: 0', 'time', 'target']\n",
    "            feature_cols = [col for col in df.columns if col not in cols_to_exclude]\n",
    "\n",
    "            # === Apply pressure difference to 'p' column ===\n",
    "            if 'p' in feature_cols:\n",
    "                df['p'] = df['p'].diff().fillna(0)\n",
    "\n",
    "            features = df[feature_cols].dropna()\n",
    "            if len(features) == 0:\n",
    "                continue\n",
    "            \n",
    "            n_complete_batches = len(features) // self.batch_size\n",
    "            if n_complete_batches == 0:\n",
    "                print(f\"File {file_id}: {len(features)} samples - No complete batches (skipped)\")\n",
    "                continue\n",
    "            \n",
    "            print(f\"File {file_id}: {len(features)} samples -> {n_complete_batches} batches\")\n",
    "            \n",
    "            for batch_idx in range(n_complete_batches):\n",
    "                start_idx = batch_idx * self.batch_size\n",
    "                end_idx = start_idx + self.batch_size\n",
    "                \n",
    "                batch_data = features.iloc[start_idx:end_idx].values\n",
    "\n",
    "                # Ensure pressure difference is preserved across batches\n",
    "                if batch_data.shape[1] >= 1:  # only if there is a pressure column\n",
    "                    batch_data[:, -1] = np.append(0, np.diff(batch_data[:, -1]))\n",
    "\n",
    "                aggregated_batch = np.concatenate([\n",
    "                    np.mean(batch_data, axis=0),\n",
    "                    np.std(batch_data, axis=0),\n",
    "                    np.min(batch_data, axis=0),\n",
    "                    np.max(batch_data, axis=0)\n",
    "                ])\n",
    "                \n",
    "                all_batches.append(aggregated_batch)\n",
    "                all_batch_labels.append(label)\n",
    "                batch_file_mapping.append(file_id)\n",
    "        \n",
    "        self.all_batches = np.array(all_batches)\n",
    "        self.all_batch_labels = np.array(all_batch_labels)\n",
    "        self.batch_file_mapping = batch_file_mapping\n",
    "        \n",
    "        print(f\"\\nTotal batches created: {len(self.all_batches)}\")\n",
    "        print(f\"Batch feature dimension: {self.all_batches.shape[1] if len(self.all_batches) > 0 else 0}\")\n",
    "\n",
    "    def split_by_files(self) -> Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray]:\n",
    "        if self.all_batches is None or len(self.all_batches) == 0:\n",
    "            raise ValueError(\"No batches created. Check if data was loaded properly.\")\n",
    "\n",
    "        all_files = list(self.file_metadata.keys())\n",
    "        np.random.seed(self.random_state)\n",
    "        np.random.shuffle(all_files)\n",
    "\n",
    "        n_train_files = int(len(all_files) * self.train_split)\n",
    "        self.train_files = all_files[:n_train_files]\n",
    "        self.test_files = all_files[n_train_files:]\n",
    "\n",
    "        print(f\"\\nTrain files: {len(self.train_files)}\")\n",
    "        print(f\"Test files: {len(self.test_files)}\")\n",
    "\n",
    "        train_indices = [i for i, file_id in enumerate(self.batch_file_mapping) if file_id in self.train_files]\n",
    "        test_indices = [i for i, file_id in enumerate(self.batch_file_mapping) if file_id in self.test_files]\n",
    "\n",
    "        X_train = self.all_batches[train_indices]\n",
    "        X_test = self.all_batches[test_indices]\n",
    "        y_train = self.all_batch_labels[train_indices]\n",
    "        y_test = self.all_batch_labels[test_indices]\n",
    "\n",
    "        self.train_batches = X_train\n",
    "        self.test_batches = X_test\n",
    "        self.train_batch_labels = y_train\n",
    "        self.test_batch_labels = y_test\n",
    "\n",
    "        print(f\"Train batches: {len(X_train)}\")\n",
    "        print(f\"Test batches: {len(X_test)}\")\n",
    "\n",
    "        self._check_activity_distribution_batches(train_indices, test_indices)\n",
    "\n",
    "        return X_train, X_test, y_train, y_test\n",
    "\n",
    "    def _check_activity_distribution_batches(self, train_indices, test_indices):\n",
    "        train_activities = set(self.all_batch_labels[train_indices])\n",
    "        test_activities = set(self.all_batch_labels[test_indices])\n",
    "        all_activities = set(self.all_batch_labels)\n",
    "\n",
    "        print(f\"Train activities: {sorted(train_activities)}\")\n",
    "        print(f\"Test activities: {sorted(test_activities)}\")\n",
    "\n",
    "        if all_activities - train_activities:\n",
    "            print(f\"Missing in train: {sorted(all_activities - train_activities)}\")\n",
    "        if all_activities - test_activities:\n",
    "            print(f\"Missing in test: {sorted(all_activities - test_activities)}\")\n",
    "\n",
    "        print(\"\\nBatch distribution per activity:\")\n",
    "        for activity in sorted(all_activities):\n",
    "            train_count = np.sum(self.all_batch_labels[train_indices] == activity)\n",
    "            test_count = np.sum(self.all_batch_labels[test_indices] == activity)\n",
    "            print(f\"{activity}: {train_count} train batches, {test_count} test batches\")\n",
    "\n",
    "    def load_data(self, root_dir: str) -> Tuple[np.ndarray, np.ndarray]:\n",
    "        if root_dir.lower() == 'train':\n",
    "            if self.train_batches is None:\n",
    "                raise ValueError(\"Call get_train_test_split first.\")\n",
    "            return self.train_batches, self.train_batch_labels\n",
    "        elif root_dir.lower() == 'test':\n",
    "            if self.test_batches is None:\n",
    "                raise ValueError(\"Call get_train_test_split first.\")\n",
    "            return self.test_batches, self.test_batch_labels\n",
    "        else:\n",
    "            raise ValueError(\"Invalid root_dir. Use 'train' or 'test'.\")\n",
    "\n",
    "    def get_train_test_split(self, train_dir: str = 'data_received_/recorded_data_12344', test_dir: str = 'data_received_/recorded_data_12345') -> Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray]:\n",
    "        self.load_all_data(train_dir, test_dir)\n",
    "        return self.split_by_files()\n",
    "\n",
    "loader = DataLoader(train_split=0.8, random_state=38, batch_size=51)\n",
    "X_train_batches, X_test_batches, y_train_batches, y_test_batches = loader.get_train_test_split('data_received/recorded_data_12344', 'data_received/recorded_data_12345')\n",
    "\n",
    "print(f\"\\nFinal Results:\")\n",
    "print(f\"Train batches shape: {X_train_batches.shape}\")\n",
    "print(f\"Test batches shape: {X_test_batches.shape}\")\n",
    "print(f\"Features per batch: {X_train_batches.shape[1] if len(X_train_batches) > 0 else 0}\")\n",
    "print(f\"Classes: {len(np.unique(y_train_batches)) if len(y_train_batches) > 0 else 0}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baa50545-e086-44c7-a43b-b98929a4be33",
   "metadata": {},
   "source": [
    "Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5cdd1bd3-5fe2-4c05-b64a-127ba0114c0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing training data:\n",
      "X_train shape: (1007, 40), y_train shape: (1007,)\n",
      "Processed X_train shape: (1007, 40)\n",
      "Unique classes: 5\n",
      "\n",
      "Processing testing data:\n",
      "X_test shape: (299, 40), y_test shape: (299,)\n",
      "Processed X_test shape: (299, 40)\n",
      "\n",
      "Final processed shapes:\n",
      "Processed X_train: (1007, 40)\n",
      "Processed X_test: (299, 40)\n"
     ]
    }
   ],
   "source": [
    "class DataProcessor:\n",
    "    def __init__(self, random_state: int = 43):\n",
    "        self.random_state = random_state\n",
    "        self.scaler = StandardScaler()\n",
    "        self.label_encoder = LabelEncoder()\n",
    "\n",
    "    def fit_transform_train(self, X_train: np.ndarray, y_train: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:\n",
    "        print(f\"\\nProcessing training data:\")\n",
    "        print(f\"X_train shape: {X_train.shape}, y_train shape: {y_train.shape}\")\n",
    "\n",
    "        if np.isnan(X_train).any():\n",
    "            print(\"Warning: NaN values found in training data, filling with 0\")\n",
    "            X_train = np.nan_to_num(X_train, 0)\n",
    "\n",
    "        shuffled_indices = shuffle(range(len(X_train)), random_state=self.random_state)\n",
    "        X_shuffled = X_train[shuffled_indices]\n",
    "        y_shuffled = y_train[shuffled_indices]\n",
    "\n",
    "        y_encoded = self.label_encoder.fit_transform(y_shuffled)\n",
    "        X_scaled = self.scaler.fit_transform(X_shuffled)\n",
    "        \n",
    "        print(f\"Processed X_train shape: {X_scaled.shape}\")\n",
    "        print(f\"Unique classes: {len(np.unique(y_encoded))}\")\n",
    "        \n",
    "        return X_scaled, y_encoded  # ✅ fixed here\n",
    "\n",
    "    def transform_test(self, X_test: np.ndarray, y_test: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:\n",
    "        print(f\"\\nProcessing testing data:\")\n",
    "        print(f\"X_test shape: {X_test.shape}, y_test shape: {y_test.shape}\")\n",
    "\n",
    "        if np.isnan(X_test).any():\n",
    "            print(\"Warning: NaN values found in testing data, filling with 0\")\n",
    "            X_test = np.nan_to_num(X_test, 0)\n",
    "\n",
    "        X_scaled = self.scaler.transform(X_test)\n",
    "        y_encoded = self.label_encoder.transform(y_test) \n",
    "        \n",
    "        print(f\"Processed X_test shape: {X_scaled.shape}\")\n",
    "        \n",
    "        return X_scaled, y_encoded\n",
    "\n",
    "processor = DataProcessor()\n",
    "\n",
    "if len(X_train_batches) > 0 and len(X_test_batches) > 0:\n",
    "    X_train_processed, y_train_processed = processor.fit_transform_train(X_train_batches, y_train_batches)\n",
    "    X_test_processed, y_test_processed = processor.transform_test(X_test_batches, y_test_batches)\n",
    "\n",
    "    print(f\"\\nFinal processed shapes:\")\n",
    "    print(f\"Processed X_train: {X_train_processed.shape}\")\n",
    "    print(f\"Processed X_test: {X_test_processed.shape}\")\n",
    "else:\n",
    "    print(\"No batches created - check your data and batch_size parameters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3957b79-1ab5-4ab8-9df0-8d278a6e79b2",
   "metadata": {},
   "source": [
    "Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b66eebf3-66b2-4f8f-931d-ec045fac37a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_statistical_features(X):\n",
    "    features = np.column_stack([\n",
    "        np.mean(X, axis=1), np.std(X, axis=1), np.var(X, axis=1),\n",
    "        np.median(X, axis=1), np.min(X, axis=1), np.max(X, axis=1),\n",
    "        np.percentile(X, 25, axis=1), np.percentile(X, 75, axis=1),\n",
    "        stats.skew(X, axis=1), stats.kurtosis(X, axis=1)\n",
    "    ])\n",
    "    return features\n",
    "\n",
    "def extract_frequency_features(X):\n",
    "    fft_vals = np.abs(fft(X, axis=1))\n",
    "    n_half = X.shape[1] // 2\n",
    "    fft_vals = fft_vals[:, :n_half]\n",
    "    \n",
    "    features = np.column_stack([\n",
    "        np.sum(fft_vals, axis=1), np.mean(fft_vals, axis=1),\n",
    "        np.std(fft_vals, axis=1), np.max(fft_vals, axis=1)\n",
    "    ])\n",
    "    return features\n",
    "\n",
    "def extract_temporal_features(X):\n",
    "    zero_crossings = np.sum(np.diff(np.sign(X), axis=1) != 0, axis=1)\n",
    "    first_diff = np.diff(X, axis=1)\n",
    "    \n",
    "    features = np.column_stack([\n",
    "        zero_crossings,\n",
    "        np.mean(first_diff, axis=1),\n",
    "        np.std(first_diff, axis=1),\n",
    "        np.sum(X**2, axis=1)\n",
    "    ])\n",
    "    return features\n",
    "\n",
    "def extract_all_features(X):\n",
    "    stat_features = extract_statistical_features(X)\n",
    "    freq_features = extract_frequency_features(X)\n",
    "    temp_features = extract_temporal_features(X)\n",
    "    return np.column_stack([stat_features, freq_features, temp_features])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ef13c42a-eb0e-49f5-813f-16402b707d94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Logistic Regression] Training features extracted: (1007, 18)\n",
      "[Logistic Regression] Test features extracted: (299, 18)\n",
      "[Logistic Regression] Training features: (1007, 58)\n",
      "[Logistic Regression] Test features: (299, 58)\n"
     ]
    }
   ],
   "source": [
    "X_train_lr_features = extract_all_features(X_train_processed)\n",
    "X_test_lr_features = extract_all_features(X_test_processed)\n",
    "\n",
    "print(f\"[Logistic Regression] Training features extracted: {X_train_lr_features.shape}\")\n",
    "print(f\"[Logistic Regression] Test features extracted: {X_test_lr_features.shape}\")\n",
    "\n",
    "X_train_lr_combined = np.column_stack([X_train_processed, X_train_lr_features])\n",
    "X_test_lr_combined = np.column_stack([X_test_processed, X_test_lr_features])\n",
    "\n",
    "X_train_lr_combined = np.nan_to_num(X_train_lr_combined, nan=0.0, posinf=1e6, neginf=-1e6)\n",
    "X_test_lr_combined = np.nan_to_num(X_test_lr_combined, nan=0.0, posinf=1e6, neginf=-1e6)\n",
    "\n",
    "print(f\"[Logistic Regression] Training features: {X_train_lr_combined.shape}\")\n",
    "print(f\"[Logistic Regression] Test features: {X_test_lr_combined.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d07dcd5-f24a-4a48-9878-6d51da4bcc6f",
   "metadata": {},
   "source": [
    "Feature Naming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1c8589e5-1364-4d3a-a1fe-367aab2dc891",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature Index:\n",
      " 0: ax_mean\n",
      " 1: ay_mean\n",
      " 2: az_mean\n",
      " 3: gx_mean\n",
      " 4: gy_mean\n",
      " 5: gz_mean\n",
      " 6: mx_mean\n",
      " 7: my_mean\n",
      " 8: mz_mean\n",
      " 9: p_mean\n",
      "10: ax_std\n",
      "11: ay_std\n",
      "12: az_std\n",
      "13: gx_std\n",
      "14: gy_std\n",
      "15: gz_std\n",
      "16: mx_std\n",
      "17: my_std\n",
      "18: mz_std\n",
      "19: p_std\n",
      "20: ax_min\n",
      "21: ay_min\n",
      "22: az_min\n",
      "23: gx_min\n",
      "24: gy_min\n",
      "25: gz_min\n",
      "26: mx_min\n",
      "27: my_min\n",
      "28: mz_min\n",
      "29: p_min\n",
      "30: ax_max\n",
      "31: ay_max\n",
      "32: az_max\n",
      "33: gx_max\n",
      "34: gy_max\n",
      "35: gz_max\n",
      "36: mx_max\n",
      "37: my_max\n",
      "38: mz_max\n",
      "39: p_max\n",
      "40: Mean\n",
      "41: Std\n",
      "42: Variance\n",
      "43: Median\n",
      "44: Min\n",
      "45: Max\n",
      "46: 25th Percentile\n",
      "47: 75th Percentile\n",
      "48: Skewness\n",
      "49: Kurtosis\n",
      "50: FFT Sum\n",
      "51: FFT Mean\n",
      "52: FFT Std\n",
      "53: FFT Max\n",
      "54: Zero Crossings\n",
      "55: First Diff Mean\n",
      "56: First Diff Std\n",
      "57: Signal Energy\n"
     ]
    }
   ],
   "source": [
    "sensor_channels = ['ax', 'ay', 'az', 'gx', 'gy', 'gz', 'mx', 'my', 'mz', 'p']\n",
    "original_feature_names = [f\"{feat}_{agg}\" for agg in ['mean', 'std', 'min', 'max'] for feat in sensor_channels]\n",
    "stat_feature_names = ['Mean', 'Std', 'Variance', 'Median', 'Min', 'Max', '25th Percentile', '75th Percentile', 'Skewness', 'Kurtosis']\n",
    "freq_feature_names = ['FFT Sum', 'FFT Mean', 'FFT Std', 'FFT Max']\n",
    "temp_feature_names = ['Zero Crossings', 'First Diff Mean', 'First Diff Std', 'Signal Energy']\n",
    "\n",
    "all_feature_names = original_feature_names + stat_feature_names + freq_feature_names + temp_feature_names\n",
    "\n",
    "print(\"Feature Index:\")\n",
    "for i, name in enumerate(all_feature_names):\n",
    "    print(f\"{i:2d}: {name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6425fa6-bf17-46ad-b878-a79e98e83c92",
   "metadata": {},
   "source": [
    "Feature Selection and Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1a02adfc-133e-48da-b975-763ad08277e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LR] Features selected: 47\n",
      "[LR] Selected features: ['ax_mean', 'ay_mean', 'az_mean', 'gx_mean', 'gy_mean', 'gz_mean', 'mx_mean', 'my_mean', 'mz_mean', 'ax_std', 'ay_std', 'az_std', 'gx_std', 'gy_std', 'gz_std', 'mx_std', 'my_std', 'mz_std', 'ax_min', 'ay_min', 'az_min', 'gx_min', 'gy_min', 'gz_min', 'mx_min', 'my_min', 'mz_min', 'ax_max', 'ay_max', 'az_max', 'gx_max', 'gy_max', 'gz_max', 'mx_max', 'my_max', 'mz_max', 'Mean', 'Variance', 'Median', 'Max', '25th Percentile', '75th Percentile', 'FFT Std', 'FFT Max', 'Zero Crossings', 'First Diff Std', 'Signal Energy']\n"
     ]
    }
   ],
   "source": [
    "selector_lr = SelectKBest(mutual_info_classif, k=47)\n",
    "X_train_lr_selected = selector_lr.fit_transform(X_train_lr_combined, y_train_processed)\n",
    "X_test_lr_selected = selector_lr.transform(X_test_lr_combined)\n",
    "\n",
    "print(f\"[LR] Features selected: {X_train_lr_selected.shape[1]}\")\n",
    "\n",
    "selected_mask_lr = selector_lr.get_support()\n",
    "selected_feature_names_lr = [name for i, name in enumerate(all_feature_names) if selected_mask_lr[i]]\n",
    "print(\"[LR] Selected features:\", selected_feature_names_lr)\n",
    "\n",
    "scaler_lr = StandardScaler()\n",
    "X_train_lr_final = scaler_lr.fit_transform(X_train_lr_selected)\n",
    "X_test_lr_final = scaler_lr.transform(X_test_lr_selected)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed30456f-2983-4e83-89fb-9eb16831ca62",
   "metadata": {},
   "source": [
    "Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "705a1752-95fa-4c1e-9906-ef10e3beea29",
   "metadata": {},
   "source": [
    "Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "85d46d54-5b95-447c-ac1a-e013cf992a35",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {\n",
       "  /* Definition of color scheme common for light and dark mode */\n",
       "  --sklearn-color-text: black;\n",
       "  --sklearn-color-line: gray;\n",
       "  /* Definition of color scheme for unfitted estimators */\n",
       "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
       "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
       "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
       "  --sklearn-color-unfitted-level-3: chocolate;\n",
       "  /* Definition of color scheme for fitted estimators */\n",
       "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
       "  --sklearn-color-fitted-level-1: #d4ebff;\n",
       "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
       "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
       "\n",
       "  /* Specific color for light theme */\n",
       "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
       "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-icon: #696969;\n",
       "\n",
       "  @media (prefers-color-scheme: dark) {\n",
       "    /* Redefinition of color scheme for dark theme */\n",
       "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
       "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-icon: #878787;\n",
       "  }\n",
       "}\n",
       "\n",
       "#sk-container-id-1 {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 pre {\n",
       "  padding: 0;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-hidden--visually {\n",
       "  border: 0;\n",
       "  clip: rect(1px 1px 1px 1px);\n",
       "  clip: rect(1px, 1px, 1px, 1px);\n",
       "  height: 1px;\n",
       "  margin: -1px;\n",
       "  overflow: hidden;\n",
       "  padding: 0;\n",
       "  position: absolute;\n",
       "  width: 1px;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-dashed-wrapped {\n",
       "  border: 1px dashed var(--sklearn-color-line);\n",
       "  margin: 0 0.4em 0.5em 0.4em;\n",
       "  box-sizing: border-box;\n",
       "  padding-bottom: 0.4em;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-container {\n",
       "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
       "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
       "     so we also need the `!important` here to be able to override the\n",
       "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
       "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
       "  display: inline-block !important;\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-text-repr-fallback {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       "div.sk-parallel-item,\n",
       "div.sk-serial,\n",
       "div.sk-item {\n",
       "  /* draw centered vertical line to link estimators */\n",
       "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
       "  background-size: 2px 100%;\n",
       "  background-repeat: no-repeat;\n",
       "  background-position: center center;\n",
       "}\n",
       "\n",
       "/* Parallel-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item::after {\n",
       "  content: \"\";\n",
       "  width: 100%;\n",
       "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
       "  flex-grow: 1;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel {\n",
       "  display: flex;\n",
       "  align-items: stretch;\n",
       "  justify-content: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:first-child::after {\n",
       "  align-self: flex-end;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:last-child::after {\n",
       "  align-self: flex-start;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:only-child::after {\n",
       "  width: 0;\n",
       "}\n",
       "\n",
       "/* Serial-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-serial {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  align-items: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  padding-right: 1em;\n",
       "  padding-left: 1em;\n",
       "}\n",
       "\n",
       "\n",
       "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
       "clickable and can be expanded/collapsed.\n",
       "- Pipeline and ColumnTransformer use this feature and define the default style\n",
       "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
       "*/\n",
       "\n",
       "/* Pipeline and ColumnTransformer style (default) */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable {\n",
       "  /* Default theme specific background. It is overwritten whether we have a\n",
       "  specific estimator or a Pipeline/ColumnTransformer */\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "/* Toggleable label */\n",
       "#sk-container-id-1 label.sk-toggleable__label {\n",
       "  cursor: pointer;\n",
       "  display: block;\n",
       "  width: 100%;\n",
       "  margin-bottom: 0;\n",
       "  padding: 0.5em;\n",
       "  box-sizing: border-box;\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:before {\n",
       "  /* Arrow on the left of the label */\n",
       "  content: \"▸\";\n",
       "  float: left;\n",
       "  margin-right: 0.25em;\n",
       "  color: var(--sklearn-color-icon);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "/* Toggleable content - dropdown */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content {\n",
       "  max-height: 0;\n",
       "  max-width: 0;\n",
       "  overflow: hidden;\n",
       "  text-align: left;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content pre {\n",
       "  margin: 0.2em;\n",
       "  border-radius: 0.25em;\n",
       "  color: var(--sklearn-color-text);\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted pre {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
       "  /* Expand drop-down */\n",
       "  max-height: 200px;\n",
       "  max-width: 100%;\n",
       "  overflow: auto;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
       "  content: \"▾\";\n",
       "}\n",
       "\n",
       "/* Pipeline/ColumnTransformer-specific style */\n",
       "\n",
       "#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator-specific style */\n",
       "\n",
       "/* Colorize estimator box */\n",
       "#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label label.sk-toggleable__label,\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  /* The background is the default theme color */\n",
       "  color: var(--sklearn-color-text-on-default-background);\n",
       "}\n",
       "\n",
       "/* On hover, darken the color of the background */\n",
       "#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "/* Label box, darken color on hover, fitted */\n",
       "#sk-container-id-1 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator label */\n",
       "\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  font-family: monospace;\n",
       "  font-weight: bold;\n",
       "  display: inline-block;\n",
       "  line-height: 1.2em;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label-container {\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "/* Estimator-specific */\n",
       "#sk-container-id-1 div.sk-estimator {\n",
       "  font-family: monospace;\n",
       "  border: 1px dotted var(--sklearn-color-border-box);\n",
       "  border-radius: 0.25em;\n",
       "  box-sizing: border-box;\n",
       "  margin-bottom: 0.5em;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "/* on hover */\n",
       "#sk-container-id-1 div.sk-estimator:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
       "\n",
       "/* Common style for \"i\" and \"?\" */\n",
       "\n",
       ".sk-estimator-doc-link,\n",
       "a:link.sk-estimator-doc-link,\n",
       "a:visited.sk-estimator-doc-link {\n",
       "  float: right;\n",
       "  font-size: smaller;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1em;\n",
       "  height: 1em;\n",
       "  width: 1em;\n",
       "  text-decoration: none !important;\n",
       "  margin-left: 1ex;\n",
       "  /* unfitted */\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted,\n",
       "a:link.sk-estimator-doc-link.fitted,\n",
       "a:visited.sk-estimator-doc-link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "/* Span, style for the box shown on hovering the info icon */\n",
       ".sk-estimator-doc-link span {\n",
       "  display: none;\n",
       "  z-index: 9999;\n",
       "  position: relative;\n",
       "  font-weight: normal;\n",
       "  right: .2ex;\n",
       "  padding: .5ex;\n",
       "  margin: .5ex;\n",
       "  width: min-content;\n",
       "  min-width: 20ex;\n",
       "  max-width: 50ex;\n",
       "  color: var(--sklearn-color-text);\n",
       "  box-shadow: 2pt 2pt 4pt #999;\n",
       "  /* unfitted */\n",
       "  background: var(--sklearn-color-unfitted-level-0);\n",
       "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted span {\n",
       "  /* fitted */\n",
       "  background: var(--sklearn-color-fitted-level-0);\n",
       "  border: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link:hover span {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link {\n",
       "  float: right;\n",
       "  font-size: 1rem;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1rem;\n",
       "  height: 1rem;\n",
       "  width: 1rem;\n",
       "  text-decoration: none;\n",
       "  /* unfitted */\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "#sk-container-id-1 a.estimator_doc_link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression(intercept_scaling=0.1, max_iter=10,\n",
       "                   multi_class=&#x27;multinomial&#x27;, n_jobs=-1, random_state=37,\n",
       "                   tol=0.001)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;&nbsp;LogisticRegression<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.5/modules/generated/sklearn.linear_model.LogisticRegression.html\">?<span>Documentation for LogisticRegression</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></label><div class=\"sk-toggleable__content fitted\"><pre>LogisticRegression(intercept_scaling=0.1, max_iter=10,\n",
       "                   multi_class=&#x27;multinomial&#x27;, n_jobs=-1, random_state=37,\n",
       "                   tol=0.001)</pre></div> </div></div></div></div>"
      ],
      "text/plain": [
       "LogisticRegression(intercept_scaling=0.1, max_iter=10,\n",
       "                   multi_class='multinomial', n_jobs=-1, random_state=37,\n",
       "                   tol=0.001)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_lr = LogisticRegression(\n",
    "    solver='lbfgs',\n",
    "    penalty='l2',\n",
    "    multi_class='multinomial',\n",
    "    C=1.0,\n",
    "    tol=1e-3,\n",
    "    max_iter=10,\n",
    "    intercept_scaling=0.1,\n",
    "    fit_intercept=True,\n",
    "    random_state=37,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "model_lr.fit(X_train_lr_final, y_train_processed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4233b142-492c-49b3-9552-813e28ae7b28",
   "metadata": {},
   "source": [
    "Evaluation and Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cf31ff1c-e814-4635-9eb5-fb900c401a62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Model Performance\n",
      "Test Accuracy: 0.826087\n",
      "Test Samples: 299\n",
      "\n",
      "Classification Report (Logistic Regression):\n",
      "              precision  recall  f1-score   support\n",
      "0                0.9697  0.9505    0.9600  101.0000\n",
      "1                0.5536  0.8611    0.6739   36.0000\n",
      "2                0.9385  0.7349    0.8243   83.0000\n",
      "3                0.9375  0.6250    0.7500   48.0000\n",
      "4                0.6170  0.9355    0.7436   31.0000\n",
      "accuracy         0.8261  0.8261    0.8261    0.8261\n",
      "macro avg        0.8033  0.8214    0.7904  299.0000\n",
      "weighted avg     0.8692  0.8261    0.8317  299.0000\n"
     ]
    }
   ],
   "source": [
    "y_pred_lr = model_lr.predict(X_test_lr_final)\n",
    "accuracy_lr = accuracy_score(y_test_processed, y_pred_lr)\n",
    "cm_lr = confusion_matrix(y_test_processed, y_pred_lr)\n",
    "classification_rep_lr = classification_report(y_test_processed, y_pred_lr, output_dict=True)\n",
    "\n",
    "print(\"Logistic Regression Model Performance\")\n",
    "print(f\"Test Accuracy: {accuracy_lr:.6f}\")\n",
    "print(f\"Test Samples: {len(y_test_processed):,}\")\n",
    "\n",
    "results_df_lr = pd.DataFrame(classification_rep_lr).transpose()\n",
    "print(\"\\nClassification Report (Logistic Regression):\")\n",
    "print(results_df_lr.round(4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39923355-daa0-4d8b-abf9-d17143cc7ba2",
   "metadata": {},
   "source": [
    "Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db05ed8a-72a9-4029-b14d-fa206ce95900",
   "metadata": {},
   "source": [
    "Data Loading and Batching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "79653ecb-91e7-469a-a30d-8a24ff150418",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total files: 29\n",
      "Activities: ['idle', 'with_hook_climbing_up', 'with_hook_descending_down', 'without_hook_climbing_up', 'without_hook_descending_down']\n",
      "\n",
      "File distribution per activity:\n",
      "idle: 4 files, 204 samples\n",
      "with_hook_climbing_up: 7 files, 357 samples\n",
      "with_hook_descending_down: 7 files, 357 samples\n",
      "without_hook_climbing_up: 5 files, 255 samples\n",
      "without_hook_descending_down: 6 files, 306 samples\n",
      "Total: 29 files, 1479 samples\n",
      "\n",
      "Creating batches with batch_size=51\n",
      "File original_test_idle_23062025_110904_idle: 51 samples -> 1 batches\n",
      "File original_test_idle_23062025_110907_idle: 51 samples -> 1 batches\n",
      "File original_test_idle_23062025_110909_idle: 51 samples -> 1 batches\n",
      "File original_test_idle_23062025_110912_idle: 51 samples -> 1 batches\n",
      "File original_test_with_hook_climbing_up_23062025_110725_idle: 51 samples -> 1 batches\n",
      "File original_test_with_hook_climbing_up_23062025_110727_idle: 51 samples -> 1 batches\n",
      "File original_test_with_hook_climbing_up_23062025_110730_idle: 51 samples -> 1 batches\n",
      "File original_test_with_hook_climbing_up_23062025_110733_idle: 51 samples -> 1 batches\n",
      "File original_test_with_hook_climbing_up_23062025_110735_idle: 51 samples -> 1 batches\n",
      "File original_test_with_hook_climbing_up_23062025_110738_idle: 51 samples -> 1 batches\n",
      "File original_test_with_hook_climbing_up_23062025_110741_idle: 51 samples -> 1 batches\n",
      "File original_test_with_hook_descending_down_23062025_110756_idle: 51 samples -> 1 batches\n",
      "File original_test_with_hook_descending_down_23062025_110759_idle: 51 samples -> 1 batches\n",
      "File original_test_with_hook_descending_down_23062025_110802_idle: 51 samples -> 1 batches\n",
      "File original_test_with_hook_descending_down_23062025_110804_idle: 51 samples -> 1 batches\n",
      "File original_test_with_hook_descending_down_23062025_110807_idle: 51 samples -> 1 batches\n",
      "File original_test_with_hook_descending_down_23062025_110810_idle: 51 samples -> 1 batches\n",
      "File original_test_with_hook_descending_down_23062025_110812_idle: 51 samples -> 1 batches\n",
      "File original_test_without_hook_climbing_up_23062025_110844_idle: 51 samples -> 1 batches\n",
      "File original_test_without_hook_climbing_up_23062025_110847_idle: 51 samples -> 1 batches\n",
      "File original_test_without_hook_climbing_up_23062025_110849_idle: 51 samples -> 1 batches\n",
      "File original_test_without_hook_climbing_up_23062025_110852_idle: 51 samples -> 1 batches\n",
      "File original_test_without_hook_climbing_up_23062025_110855_idle: 51 samples -> 1 batches\n",
      "File original_test_without_hook_descending_down_23062025_110923_idle: 51 samples -> 1 batches\n",
      "File original_test_without_hook_descending_down_23062025_110925_idle: 51 samples -> 1 batches\n",
      "File original_test_without_hook_descending_down_23062025_110928_idle: 51 samples -> 1 batches\n",
      "File original_test_without_hook_descending_down_23062025_110931_idle: 51 samples -> 1 batches\n",
      "File original_test_without_hook_descending_down_23062025_110933_idle: 51 samples -> 1 batches\n",
      "File original_test_without_hook_descending_down_23062025_110936_idle: 51 samples -> 1 batches\n",
      "\n",
      "Total batches created: 29\n",
      "Batch feature dimension: 36\n",
      "Total files: 29\n",
      "Activities: ['idle', 'with_hook_climbing_up', 'with_hook_descending_down', 'without_hook_climbing_up', 'without_hook_descending_down']\n",
      "\n",
      "File distribution per activity:\n",
      "idle: 4 files, 204 samples\n",
      "with_hook_climbing_up: 7 files, 357 samples\n",
      "with_hook_descending_down: 7 files, 357 samples\n",
      "without_hook_climbing_up: 5 files, 255 samples\n",
      "without_hook_descending_down: 6 files, 306 samples\n",
      "Total: 29 files, 1479 samples\n",
      "\n",
      "Creating batches with batch_size=51\n",
      "File original_test_idle_23062025_110904_idle: 51 samples -> 1 batches\n",
      "File original_test_idle_23062025_110907_idle: 51 samples -> 1 batches\n",
      "File original_test_idle_23062025_110909_idle: 51 samples -> 1 batches\n",
      "File original_test_idle_23062025_110912_idle: 51 samples -> 1 batches\n",
      "File original_test_with_hook_climbing_up_23062025_110725_idle: 51 samples -> 1 batches\n",
      "File original_test_with_hook_climbing_up_23062025_110727_idle: 51 samples -> 1 batches\n",
      "File original_test_with_hook_climbing_up_23062025_110730_idle: 51 samples -> 1 batches\n",
      "File original_test_with_hook_climbing_up_23062025_110733_idle: 51 samples -> 1 batches\n",
      "File original_test_with_hook_climbing_up_23062025_110735_idle: 51 samples -> 1 batches\n",
      "File original_test_with_hook_climbing_up_23062025_110738_idle: 51 samples -> 1 batches\n",
      "File original_test_with_hook_climbing_up_23062025_110741_idle: 51 samples -> 1 batches\n",
      "File original_test_with_hook_descending_down_23062025_110756_idle: 51 samples -> 1 batches\n",
      "File original_test_with_hook_descending_down_23062025_110759_idle: 51 samples -> 1 batches\n",
      "File original_test_with_hook_descending_down_23062025_110802_idle: 51 samples -> 1 batches\n",
      "File original_test_with_hook_descending_down_23062025_110804_idle: 51 samples -> 1 batches\n",
      "File original_test_with_hook_descending_down_23062025_110807_idle: 51 samples -> 1 batches\n",
      "File original_test_with_hook_descending_down_23062025_110810_idle: 51 samples -> 1 batches\n",
      "File original_test_with_hook_descending_down_23062025_110812_idle: 51 samples -> 1 batches\n",
      "File original_test_without_hook_climbing_up_23062025_110844_idle: 51 samples -> 1 batches\n",
      "File original_test_without_hook_climbing_up_23062025_110847_idle: 51 samples -> 1 batches\n",
      "File original_test_without_hook_climbing_up_23062025_110849_idle: 51 samples -> 1 batches\n",
      "File original_test_without_hook_climbing_up_23062025_110852_idle: 51 samples -> 1 batches\n",
      "File original_test_without_hook_climbing_up_23062025_110855_idle: 51 samples -> 1 batches\n",
      "File original_test_without_hook_descending_down_23062025_110923_idle: 51 samples -> 1 batches\n",
      "File original_test_without_hook_descending_down_23062025_110925_idle: 51 samples -> 1 batches\n",
      "File original_test_without_hook_descending_down_23062025_110928_idle: 51 samples -> 1 batches\n",
      "File original_test_without_hook_descending_down_23062025_110931_idle: 51 samples -> 1 batches\n",
      "File original_test_without_hook_descending_down_23062025_110933_idle: 51 samples -> 1 batches\n",
      "File original_test_without_hook_descending_down_23062025_110936_idle: 51 samples -> 1 batches\n",
      "\n",
      "Total batches created: 29\n",
      "Batch feature dimension: 36\n",
      "Shape of X_base: (58, 36)\n",
      "Unique true labels: ['idle' 'with_hook_climbing_up' 'with_hook_descending_down'\n",
      " 'without_hook_climbing_up' 'without_hook_descending_down']\n"
     ]
    }
   ],
   "source": [
    "# DATA_PARENT_DIR = \"data_received_v\"\n",
    "# VAL_DIRS = [\"recorded_data_12344\", \"recorded_data_12345\"]\n",
    "DATA_PARENT_DIR = \"x_dataset\"\n",
    "VAL_DIRS = [\"module1\", \"module2\"]\n",
    "\n",
    "all_batches = []\n",
    "all_labels = []\n",
    "\n",
    "val_loader = DataLoader(train_split=0, random_state=38, batch_size=51)\n",
    "\n",
    "for folder in VAL_DIRS:\n",
    "    full_path = os.path.join(DATA_PARENT_DIR, folder)\n",
    "    \n",
    "    val_loader.load_all_data(\"__dummy__\", full_path)  # loads and resets internal state\n",
    "    \n",
    "    all_batches.extend(val_loader.all_batches)\n",
    "    all_labels.extend(val_loader.all_batch_labels)\n",
    "\n",
    "if not all_batches:\n",
    "    raise ValueError(\"No valid batches created from input files.\")\n",
    "\n",
    "X_new = np.vstack(all_batches)\n",
    "y_true = np.array(all_labels)\n",
    "\n",
    "print(\"Shape of X_base:\", X_new.shape)\n",
    "print(\"Unique true labels:\", np.unique(y_true))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0160483e-5db1-4ed3-b4f0-837c0b8d2856",
   "metadata": {},
   "source": [
    "LR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "22f952a6-8076-4107-99a6-de9f4b3bc9c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing testing data:\n",
      "X_test shape: (58, 36), y_test shape: (58,)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "X has 36 features, but StandardScaler is expecting 40 features as input.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m X_new_proc_lr, _ = \u001b[43mprocessor\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtransform_test\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_new\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_true\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      2\u001b[39m X_new_feat_lr = extract_all_features(X_new_proc_lr)\n\u001b[32m      3\u001b[39m X_new_comb_lr = np.nan_to_num(np.column_stack([X_new_proc_lr, X_new_feat_lr]), nan=\u001b[32m0.0\u001b[39m, posinf=\u001b[32m1e6\u001b[39m, neginf=-\u001b[32m1e6\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 35\u001b[39m, in \u001b[36mDataProcessor.transform_test\u001b[39m\u001b[34m(self, X_test, y_test)\u001b[39m\n\u001b[32m     32\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mWarning: NaN values found in testing data, filling with 0\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     33\u001b[39m     X_test = np.nan_to_num(X_test, \u001b[32m0\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m35\u001b[39m X_scaled = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mscaler\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     36\u001b[39m y_encoded = \u001b[38;5;28mself\u001b[39m.label_encoder.transform(y_test) \n\u001b[32m     38\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mProcessed X_test shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mX_scaled.shape\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\utils\\_set_output.py:313\u001b[39m, in \u001b[36m_wrap_method_output.<locals>.wrapped\u001b[39m\u001b[34m(self, X, *args, **kwargs)\u001b[39m\n\u001b[32m    311\u001b[39m \u001b[38;5;129m@wraps\u001b[39m(f)\n\u001b[32m    312\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[34mwrapped\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, *args, **kwargs):\n\u001b[32m--> \u001b[39m\u001b[32m313\u001b[39m     data_to_wrap = \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    314\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data_to_wrap, \u001b[38;5;28mtuple\u001b[39m):\n\u001b[32m    315\u001b[39m         \u001b[38;5;66;03m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[32m    316\u001b[39m         return_tuple = (\n\u001b[32m    317\u001b[39m             _wrap_data_with_container(method, data_to_wrap[\u001b[32m0\u001b[39m], X, \u001b[38;5;28mself\u001b[39m),\n\u001b[32m    318\u001b[39m             *data_to_wrap[\u001b[32m1\u001b[39m:],\n\u001b[32m    319\u001b[39m         )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\preprocessing\\_data.py:1045\u001b[39m, in \u001b[36mStandardScaler.transform\u001b[39m\u001b[34m(self, X, copy)\u001b[39m\n\u001b[32m   1042\u001b[39m check_is_fitted(\u001b[38;5;28mself\u001b[39m)\n\u001b[32m   1044\u001b[39m copy = copy \u001b[38;5;28;01mif\u001b[39;00m copy \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m.copy\n\u001b[32m-> \u001b[39m\u001b[32m1045\u001b[39m X = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_validate_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1046\u001b[39m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1047\u001b[39m \u001b[43m    \u001b[49m\u001b[43mreset\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   1048\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcsr\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1049\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1050\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mFLOAT_DTYPES\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1051\u001b[39m \u001b[43m    \u001b[49m\u001b[43mforce_writeable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   1052\u001b[39m \u001b[43m    \u001b[49m\u001b[43mforce_all_finite\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mallow-nan\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1053\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1055\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m sparse.issparse(X):\n\u001b[32m   1056\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.with_mean:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\base.py:654\u001b[39m, in \u001b[36mBaseEstimator._validate_data\u001b[39m\u001b[34m(self, X, y, reset, validate_separately, cast_to_ndarray, **check_params)\u001b[39m\n\u001b[32m    651\u001b[39m     out = X, y\n\u001b[32m    653\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m check_params.get(\u001b[33m\"\u001b[39m\u001b[33mensure_2d\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[32m--> \u001b[39m\u001b[32m654\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_check_n_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreset\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    656\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\base.py:443\u001b[39m, in \u001b[36mBaseEstimator._check_n_features\u001b[39m\u001b[34m(self, X, reset)\u001b[39m\n\u001b[32m    440\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[32m    442\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m n_features != \u001b[38;5;28mself\u001b[39m.n_features_in_:\n\u001b[32m--> \u001b[39m\u001b[32m443\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    444\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mX has \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mn_features\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m features, but \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.\u001b[34m__class__\u001b[39m.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    445\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mis expecting \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.n_features_in_\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m features as input.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    446\u001b[39m     )\n",
      "\u001b[31mValueError\u001b[39m: X has 36 features, but StandardScaler is expecting 40 features as input."
     ]
    }
   ],
   "source": [
    "X_new_proc_lr, _ = processor.transform_test(X_new, y_true)\n",
    "X_new_feat_lr = extract_all_features(X_new_proc_lr)\n",
    "X_new_comb_lr = np.nan_to_num(np.column_stack([X_new_proc_lr, X_new_feat_lr]), nan=0.0, posinf=1e6, neginf=-1e6)\n",
    "\n",
    "X_new_sel_lr = selector_lr.transform(X_new_comb_lr)\n",
    "X_new_fin_lr = scaler_lr.transform(X_new_sel_lr)\n",
    "\n",
    "y_pred_lr = model_lr.predict(X_new_fin_lr)\n",
    "y_true = processor.label_encoder.transform(y_true)\n",
    "acc_lr = accuracy_score(y_true, y_pred_lr)\n",
    "\n",
    "print(\"Validation Performance - Logistic Regression\")\n",
    "print(f\"Accuracy: {acc_lr:.4f}\")\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_true, y_pred_lr, digits=4))\n",
    "\n",
    "cm_lr_val = confusion_matrix(y_true, y_pred_lr)\n",
    "classification_rep_lr_val = classification_report(y_true, y_pred_lr, output_dict=True)\n",
    "results_df_lr_val = pd.DataFrame(classification_rep_lr_val).transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "625558c8-19fc-42ca-bb13-6bc6e002d48f",
   "metadata": {},
   "source": [
    "Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ef5802b-47e9-4a03-a0e7-4d227c12b285",
   "metadata": {},
   "source": [
    "Top 10 Important Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6efc9d0b-ce73-4fc0-ba33-4c1231e81ba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(figsize=(18, 6))\n",
    "\n",
    "coef_abs = np.abs(model_lr.coef_).mean(axis=0)\n",
    "top_10_idx_lr = np.argsort(coef_abs)[-10:][::-1]\n",
    "top_10_scores_lr = coef_abs[top_10_idx_lr]\n",
    "top_10_names_lr = [all_feature_names[i] for i in top_10_idx_lr]\n",
    "\n",
    "axes.barh(np.arange(10), top_10_scores_lr, color='orange')\n",
    "axes.set_yticks(np.arange(10))\n",
    "axes.set_yticklabels(top_10_names_lr)\n",
    "axes.invert_yaxis()\n",
    "axes.set_title('Top 10 LR Important Features (Validation)', fontsize=14, fontweight='bold')\n",
    "axes.set_xlabel('LR Coefficient Magnitude')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c3dd04a-f75d-46de-9fed-3d9df404e532",
   "metadata": {},
   "source": [
    "Confusion Matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb3d3349-6c8f-4e28-8197-75ccdae867a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(figsize=(16, 6))\n",
    "\n",
    "class_names = processor.label_encoder.inverse_transform(model_lr.classes_)\n",
    "\n",
    "sns.heatmap(cm_lr_val, annot=True, fmt='d', cmap='Oranges',\n",
    "            xticklabels=class_names,\n",
    "            yticklabels=class_names,\n",
    "            ax=axes)\n",
    "axes.set_title('Confusion Matrix - LR (Validation)', fontsize=14, fontweight='bold')\n",
    "axes.set_xlabel('Predicted')\n",
    "axes.set_ylabel('Actual')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3dc4cce-1e49-4b3e-b883-4cb1bb8284ef",
   "metadata": {},
   "source": [
    "Per-Class Performance Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b6bbad3-40e8-4d0a-a675-6698e3c29e21",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "results_df_lr_val.index = results_df_lr_val.index.map(lambda x: int(x) if str(x).isdigit() else x)\n",
    "\n",
    "index_to_label = {\n",
    "    0: 'without_hook_descending_down',\n",
    "    1: 'without_hook_climbing_up',\n",
    "    2: 'with_hook_descending_down',\n",
    "    3: 'with_hook_climbing_up',\n",
    "    4: 'idle'\n",
    "}\n",
    "\n",
    "class_indices = list(index_to_label.keys())\n",
    "class_names = [index_to_label[i] for i in class_indices]\n",
    "\n",
    "metrics = ['precision', 'recall', 'f1-score']\n",
    "x_pos = np.arange(len(class_names))\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "for i, metric in enumerate(metrics):\n",
    "    values = [results_df_lr_val.loc[class_idx, metric] for class_idx in class_indices]\n",
    "    ax.bar(x_pos + i * 0.25, values, width=0.25, label=metric)\n",
    "\n",
    "    for j, val in enumerate(values):\n",
    "        ax.text(x_pos[j] + i * 0.25, val + 0.01, f\"{val:.2f}\", ha='center', va='bottom', fontsize=8)\n",
    "\n",
    "ax.set_title(\"Per-Class Metrics - LR (Validation)\", fontsize=14, fontweight='bold')\n",
    "ax.set_xlabel(\"Classes\")\n",
    "ax.set_ylabel(\"Score\")\n",
    "ax.set_xticks(x_pos + 0.25)\n",
    "ax.set_xticklabels(class_names, rotation=45)\n",
    "ax.legend()\n",
    "ax.set_ylim(0, 1)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c731980e-e87f-4032-b1ee-ca42ea7ed86a",
   "metadata": {},
   "source": [
    "Saving Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1a42a7cd-4742-44c6-8056-af68c49cadb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression Model Size: 2.76 KB\n"
     ]
    }
   ],
   "source": [
    "joblib.dump(processor.label_encoder, \"model_files/label_encoder.pkl\")  \n",
    "joblib.dump(model_lr, \"model_files/model_lr.pkl\")\n",
    "joblib.dump(processor.scaler, \"model_files/scaler_aggregated.pkl\")\n",
    "joblib.dump(selector_lr, \"model_files/selector_lr.pkl\")\n",
    "joblib.dump(scaler_lr, \"model_files/scaler_lr.pkl\")\n",
    "\n",
    "size_lr_kb = os.path.getsize(\"model_files/model_lr.pkl\") / 1024\n",
    "print(f\"Logistic Regression Model Size: {size_lr_kb:.2f} KB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b5107b1-ddc3-40aa-82b5-cf5b1ea92e49",
   "metadata": {},
   "source": [
    "Live Data Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "22146b6c-fbdc-4a30-98e8-f52035446d4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import glob  # <-- Add this\n",
    "\n",
    "# DATA_PARENT_DIR = \"live_testing_data_F\"\n",
    "# VAL_DIRS = [\"module1\", \"module2\"]\n",
    "# headers = [\"ax\", \"ay\", \"az\", \"gx\", \"gy\", \"gz\", \"mx\", \"my\", \"mz\", \"p\"]\n",
    "# overwrite = False  \n",
    "# output_root = \"live_testing_data_\" \n",
    "\n",
    "# for folder in VAL_DIRS:\n",
    "#     input_path = os.path.join(DATA_PARENT_DIR, folder)\n",
    "#     output_path = input_path if overwrite else os.path.join(output_root, folder)\n",
    "#     all_csv_files = glob.glob(os.path.join(input_path, \"**/*.csv\"), recursive=True)\n",
    "\n",
    "#     for full_input_path in all_csv_files:\n",
    "#         rel_path = os.path.relpath(full_input_path, input_path)\n",
    "#         full_output_path = os.path.join(output_path, rel_path)\n",
    "#         os.makedirs(os.path.dirname(full_output_path), exist_ok=True)\n",
    "\n",
    "#         try:\n",
    "#             df = pd.read_csv(full_input_path, header=None, encoding=\"ISO-8859-1\")\n",
    "\n",
    "#             if df.shape[1] != len(headers):\n",
    "#                 print(f\"Skipping {rel_path}: expected {len(headers)} columns, got {df.shape[1]}\")\n",
    "#                 continue\n",
    "\n",
    "#             df.columns = headers\n",
    "#             df.to_csv(full_output_path, index=False, encoding=\"ISO-8859-1\")\n",
    "\n",
    "#         except Exception as e:\n",
    "#             print(f\"Error processing {full_input_path}: {e}\")\n",
    "\n",
    "# all_batches = []\n",
    "# all_labels = []\n",
    "\n",
    "# val_loader = DataLoader(train_split=0, random_state=38, batch_size=51)\n",
    "\n",
    "# for folder in VAL_DIRS:\n",
    "#     full_path = os.path.join(output_root if not overwrite else DATA_PARENT_DIR, folder)\n",
    "#     val_loader.load_all_data(\"__dummy__\", full_path)\n",
    "#     all_batches.extend(val_loader.all_batches)\n",
    "#     all_labels.extend(val_loader.all_batch_labels)\n",
    "\n",
    "# if not all_batches:\n",
    "#     raise ValueError(\"No valid batches created from input files.\")\n",
    "\n",
    "# X_new = np.vstack(all_batches)\n",
    "# y_true = np.array(all_labels)\n",
    "\n",
    "# print(\"Shape of X_base:\", X_new.shape)\n",
    "# print(\"Unique true labels:\", np.unique(y_true))\n",
    "\n",
    "# X_new_proc_lr, _ = processor.transform_test(X_new, y_true)\n",
    "# X_new_feat_lr = extract_all_features(X_new_proc_lr)\n",
    "# X_new_comb_lr = np.nan_to_num(\n",
    "#     np.column_stack([X_new_proc_lr, X_new_feat_lr]),\n",
    "#     nan=0.0, posinf=1e6, neginf=-1e6\n",
    "# )\n",
    "\n",
    "# X_new_sel_lr = selector_lr.transform(X_new_comb_lr)\n",
    "# X_new_fin_lr = scaler_lr.transform(X_new_sel_lr)\n",
    "\n",
    "# y_pred_lr = model_lr.predict(X_new_fin_lr)\n",
    "# y_true_enc = processor.label_encoder.transform(y_true)\n",
    "\n",
    "# acc_lr = accuracy_score(y_true_enc, y_pred_lr)\n",
    "\n",
    "# print(\"\\nValidation Performance - Logistic Regression\")\n",
    "# print(f\"Accuracy: {acc_lr:.4f}\")\n",
    "# print(\"Classification Report:\")\n",
    "# print(classification_report(y_true_enc, y_pred_lr, digits=4))\n",
    "\n",
    "# cm_lr_liv = confusion_matrix(y_true_enc, y_pred_lr)\n",
    "# classification_rep_lr_val = classification_report(y_true_enc, y_pred_lr, output_dict=True)\n",
    "# results_df_lr_val = pd.DataFrame(classification_rep_lr_val).transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38a58eea-e5e3-496f-9f5e-10f95c940934",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
